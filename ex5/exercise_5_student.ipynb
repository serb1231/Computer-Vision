{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5 (Hands-on: Convolutional Neural Networks for Image Segementation) - (15 Pts)\n",
    "\n",
    "In this homework, we will explore segmentation and data preprocessing with convolutional neural networks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conda Setup\n",
    "\n",
    "Generate the conda environment by running in the terminal:\n",
    "\n",
    "`conda create --name gcv_exercise_5 python=3.8`\n",
    "\n",
    "Run to activate environment:\n",
    "\n",
    "`conda activate gcv_exercise_5`\n",
    "\n",
    "Install all required packages for this exercise by running:\n",
    "\n",
    "`conda install matplotlib numpy scikit-image scikit-learn tqdm pandas`\n",
    "\n",
    "Install PyTorch and its prerequisites:\n",
    "\n",
    "`conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia`\n",
    "\n",
    "Install Albumentations, pandas, seaborn and ipykernel for the preprocessing\n",
    "\n",
    "`conda install -c conda-forge albumentations pandas ipykernel seaborn`\n",
    "\n",
    "(Optional) If you are working on the TUWEL Juypter Notebooks:\n",
    "\n",
    "`ipython kernel install --name \"GCV_E_5\" --user`\n",
    "\n",
    "## Notebook Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import math\n",
    "import random\n",
    "from random import randint\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import albumentations as A # Scikit-Image and -Learn library for image augmentation\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt # useful plotting library for python\n",
    "import seaborn as sns # useful plotting library for python\n",
    "import torch # framework for deep learning\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# This code is to make matplotlib figures appear inline in the notebook rather than in a new window.\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define constants for masks\n",
    "\n",
    "MASK_FG = 0\n",
    "MASK_BG = 1\n",
    "MASK_IGNORE = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data preparation\n",
    "\n",
    "When it comes to deep learning, the quality and quantity of our data can make or break the performance of our model. Therefore, in this section, we'll be preparing our dataset in a format that is most suitable for use in the remainder of our framework.\n",
    "\n",
    "For image-based tasks, two commonly used packages for data preparation and transformation are torchvision and albumentations. These packages provide a range of data loaders for common datasets and various image transformation operations that we can use to preprocess our data.\n",
    "\n",
    "Specifically, we'll be working with the oxford-iiit-pet dataset in this section. Our aim is to apply a series of data augmentations to the images to increase the diversity and quantity of our training data, while also normalizing the images to facilitate more effective model training.\n",
    "\n",
    "To perform these augmentations, we'll utilize the functions provided by the torchvision and albumentations packages. The input images can be found in the data/oxford-iiit-pet/images/ directory, while the corresponding masks are located in the data/oxford-iiit-pet/annotations/trimaps/ directory. Note that all images should be resized to 64x64 after the augmentation step. The dataset will be automatically downloaded by the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.datasets.OxfordIIITPet(\"data\", download=True)\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"data/oxford-iiit-pet/annotations/list.txt\",\n",
    "    delimiter=\" \",\n",
    "    skiprows=6,\n",
    "    header=None,\n",
    "    names=[\"stem\", \"class_id\", \"species\", \"breed\"]\n",
    ")\n",
    "df[\"class_name\"] = df.stem.map(lambda x: x.split(\"_\")[0])\n",
    "df[\"image\"] = df.stem.map(lambda x: f\"data/oxford-iiit-pet/images/{x}.jpg\")\n",
    "df[\"trimap\"] = df.stem.map(lambda x: f\"data/oxford-iiit-pet/annotations/trimaps/{x}.png\")\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize dataset information\n",
    "\n",
    "Before diving into the model training phase, it's essential to gain a high-level overview of the data we're working with. This can be achieved using visualization tools such as Matplotlib and Seaborn. Seaborn, in particular, is a popular Python data visualization library that offers a user-friendly interface for creating informative and aesthetically pleasing statistical graphics.\n",
    "\n",
    "By visualizing the data in this way, we can identify patterns and trends in the input images and labels, which can help us make informed decisions about our model design and training strategy. The resulting plot should resemble the following visualization:\n",
    "\n",
    "<center><img src=\"imgs/example_data_visualization.png\" alt=\"Plot\" width=\"960\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "# *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "# the countplot to make the rectangles, ax, to specify that they are in the same plot\n",
    "sns.countplot(x='class_id', data=df, ax=axs[0])\n",
    "sns.countplot(x='species', data=df, ax=axs[1])\n",
    "sns.countplot(x='breed', data=df, ax=axs[2])\n",
    "plt.show()\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize dataset example\n",
    "\n",
    "Now, let's take a moment to visualize a sample from our training dataset. To start, we'll focus on the first image in the dataset, which should give us a good idea of what the training data looks like.\n",
    "\n",
    "By examining the training data in more detail, we can better understand the characteristics of the input images and the corresponding labels, which can inform our model design and hyperparameter selection. So, without further ado, let's take a look at the first image in the dataset. It should resemble the following visualization:\n",
    "\n",
    "<center><img src=\"imgs/example_image.png\" alt=\"Plot\" width=\"960\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0])\n",
    "# *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "\n",
    "image1 = Image.open(df.iloc[0]['image'])\n",
    "image2 = Image.open(df.iloc[0]['trimap'])\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "# we had to deactivate the axis\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(image1)\n",
    "plt.axis('off')\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(image2)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "# *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the dataloader\n",
    "\n",
    "Moving forward, we'll be creating a dataloader to streamline the process of loading and augmenting our data. To do this, we'll need to implement the __getitem__ function and define the specific image augmentation techniques we'll be using. Please note, the images may not all be RGB, so you have to convert them to RGB (.convert('RGB')). \n",
    "\n",
    "The augmentations we'll be applying during training are:\n",
    "- Flipping the image horizontally and vertically\n",
    "- Randomly scaling the image\n",
    "- Randomly rotating the image\n",
    "- Random brightness and contrast\n",
    "- Resizing the image to 64 x 64\n",
    "- Normalization with the imagenet mean and std\n",
    "\n",
    "To ensure a fair and unbiased evaluation, it is important to keep the validation set representative of the original data distribution without introducing any artificial modifications. This allows us to accurately assess the model's performance on unseen data and make reliable comparisons between different models or approaches. There for we will only apply the resizing and the normalization to the imagenet mean and std. The output of the augmented image should be a pytorch tensor.\n",
    "\n",
    "By automating the image augmentation process, we can efficiently generate a diverse range of training samples, which in turn helps to improve the robustness and generalization of our model. So, let's get started by defining the __getitem__ function and specifying the desired data augmentation methods.\n",
    "\n",
    "Note: The trimap has three pixel annotations: 1: Foreground, 2: Background, 3: Not classified. For training you have to adapt these, such that they start with 0. (0: Foreground, 1: Background, 2: Not classified)\n",
    "\n",
    "Note: the `nn.CrossEntropyLoss()` loss expects the mask to be of type long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IIITDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"IIIT dataset.\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Pandas dataframe obtained from the Oxford-IIIT Pet Dataset.\n",
    "        tfm (albumentations.Compose): composed albumentations transforms.\n",
    "\n",
    "    Methods:\n",
    "        __len__: returns the length of the dataset.\n",
    "        __getitem__: returns a tuple of (image, mask) where mask is a 2D numpy array.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, tfm=None):\n",
    "        \"\"\"Initialize the dataset.\n",
    "\n",
    "        Args:\n",
    "            df (pandas.DataFrame): Pandas dataframe obtained from the Oxford-IIIT Pet Dataset.\n",
    "            tfm (albumentations.Compose): composed albumentations transforms.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.tfm = tfm\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the dataset.\"\"\"\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"Return a tuple of (image, mask) where mask is a 2D numpy array.\n",
    "        \n",
    "        Args:\n",
    "            i (int): index of the image in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, mask) where mask is a 2D numpy array.\n",
    "        \"\"\"\n",
    "        # *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "        \n",
    "        image = Image.open(self.df.iloc[i]['image']).convert('RGB')\n",
    "        mask = Image.open(self.df.iloc[i]['trimap'])\n",
    "\n",
    "        # as it needs to start from 0, we decrement by 1\n",
    "        image = np.array(image)\n",
    "        mask = np.array(mask) - 1 \n",
    "        \n",
    "        # apply the augmentations\n",
    "        augmentation = self.tfm(image=image, mask=mask)\n",
    "\n",
    "        tens = ToTensorV2()(image=augmentation['image'],mask = augmentation['mask'])\n",
    "        # it is a dictionary containing the image\n",
    "        image = tens['image']\n",
    "\n",
    "        mask = tens['mask']\n",
    "        # we need a long type\n",
    "        mask = mask.long()\n",
    "        # and we also need to return a numpy\n",
    "        mask = mask.numpy()\n",
    "        return image,mask\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "\n",
    "imagenet_mean = (0.485, 0.456, 0.406)\n",
    "imagenet_std = (0.229, 0.224, 0.225)\n",
    "\n",
    "# transformations for the train and validation sets\n",
    "train_tfm = A.Compose([\n",
    "    # *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "    A.RandomRotate90(),\n",
    "    A.VerticalFlip(),\n",
    "    A.HorizontalFlip(),\n",
    "    A.RandomScale(),\n",
    "    A.RandomBrightness(),\n",
    "    A.RandomBrightnessContrast(),\n",
    "    A.Resize(height=64,width=64),\n",
    "    A.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "])\n",
    "# Only normalize, resize and convert to tensor for validation\n",
    "# to keep the validation deterministic\n",
    "val_tfm = A.Compose([\n",
    "    # *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "    A.Resize(64, 64),\n",
    "    A.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "    ToTensorV2()\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "])\n",
    "\n",
    "def imagenet_denorm(x):\n",
    "    \"\"\"x: array-like with shape (..., H, W, C)\"\"\"\n",
    "    return x * imagenet_std + imagenet_mean\n",
    "\n",
    "dataset = IIITDataset(df, tfm=train_tfm)\n",
    "\n",
    "print(len(dataset))\n",
    "\n",
    "# split dataset into train and test\n",
    "train_set, validation_set = torch.utils.data.random_split(dataset, [len(dataset) - 2000, 2000])\n",
    "\n",
    "# create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=2048, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=2048, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize augmented images with masks\n",
    "\n",
    "Before proceeding further, it's important to conduct a sanity check to verify the integrity of our data. One approach we can take is to visually inspect a sample of images and their corresponding masks from the augmented dataset. This allows us to ensure that our data augmentation process is functioning correctly and not inadvertently introducing any errors or biases into the training data.\n",
    "\n",
    "To give you an idea, here's an example of what this might look like:\n",
    "\n",
    "<center><img src=\"imgs/example_aug.png\" alt=\"Plot\" width=\"960\" /></center>\n",
    "\n",
    "Note: Matplotlib will automatically clip the image and the mask. Normalize them between 0 and 1 before plotting them. Warning: \n",
    "\n",
    "`Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize some images\n",
    "for _ in range(3):\n",
    "    # *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "\n",
    "    df_length = len(df)\n",
    "    img, mask = IIITDataset(df=df, tfm=train_tfm).__getitem__(random.randint(0, df_length))\n",
    "\n",
    "    # as it has 3 channels\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    img = img - np.min(img)\n",
    "    norm_img = img / np.max(img)\n",
    "    mask = mask - np.min(mask)\n",
    "    norm_mask = mask / np.max(mask)\n",
    "\n",
    "    # just showing the figure\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(norm_img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(norm_mask, cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing the network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "\n",
    "We use a pretrained MobileNetV2 as our encoder and then implement our own decoder. We return three different layers for the skip connections of our \"U-Net\". You will implement the U-Net-Decoder and then train it on the dataset. An example overview of a complete U-Net architecture is presented here:\n",
    "\n",
    "<center><img src=\"imgs/unet.png\" alt=\"Plot\" width=\"960\" /></center>\n",
    "\n",
    "However, our U-Net will be simpler, to reduce training time, and will only have 3 down- and upsampling steps and therefore, only two skip connections (the last upsampling is done via bilinear interpolation, to futher speed up training). For the implementation you will need these torch.nn functions:\n",
    "\n",
    "- nn.Conv2d          (https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
    "- nn.BatchNorm2d     (https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)\n",
    "- nn.ReLU6           (https://pytorch.org/docs/stable/generated/torch.nn.ReLU6.html)\n",
    "- nn.ConvTranspose2d (https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html)\n",
    "\n",
    "ReLU6 is a modification of the `rectified linear unit` where we limit the activation to a maximum size of 6.\n",
    "\n",
    "The final architecture should look similar to this:\n",
    "\n",
    "- `nn.ConvTranspose2d`(kernel_size=(4,4), stride=2, padding=1, bias=True)\n",
    "- `InvertedResidual`(stride=1, expand_ratio=6)\n",
    "- `nn.ConvTranspose2d`(kernel_size=(4,4), stride=2, padding=1, bias=True)\n",
    "- `InvertedResidual`(stride=1, expand_ratio=6)\n",
    "- `nn.Conv2d`(kernel_size=(1,1))\n",
    "- `nn.Conv2d`(kernel_size=(1,1))\n",
    "- `torch.sigmoid`\n",
    "\n",
    "where InvertedResidual is composed of:\n",
    "\n",
    "- `nn.Conv2d`(kernel_size=(1,1), stride=1, padding=0, bias=False)\n",
    "- `nn.BatchNorm2d`\n",
    "- `nn.ReLU6`\n",
    "- `nn.Conv2d`(kernel_size=(3,3), stride=stride, padding=1, bias=False)\n",
    "- `nn.BatchNorm2d`\n",
    "- `nn.ReLU6`\n",
    "- `nn.Conv2d`(kernel_size=(1,1), stride=1, padding=0, bias=False)\n",
    "- `nn.BatchNorm2d`\n",
    "\n",
    "The dimensions of the layers can be seen here:\n",
    "\n",
    "| Layer (type)               | Output Shape (B, C, H, W) |\n",
    "|----------------------------|---------------------------|\n",
    "| ├─MobileNetV2_Encoder: 1-1 | [64, 32, 8, 8]\n",
    "| ├─UNet_Decoder: 1-2        | [64, 3, 32, 32]           |\n",
    "| │  └─ConvTranspose2d: 2-2  | [64, 24, 8, 8]            |\n",
    "| │  └─InvertedResidual: 2-3 | [64, 24, 8, 8]            |\n",
    "| │  └─ConvTranspose2d: 2-4  | [64, 16, 16, 16]          |\n",
    "| │  └─InvertedResidual: 2-5 | [64, 16, 16, 16]          |\n",
    "| │  └─Conv2d: 2-6           | [64, 16, 32, 32]          |\n",
    "| │  └─Conv2d: 2-7           | [64, 3, 32, 32]           |\n",
    "\n",
    "where B is the batchsize, C is the number of channels, and H and W are the height and width, respectively. The hidden dimensions within the InvertedResidual layer are defined by `hidden_dim = round(inp * expand_ratio)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "\n",
    "        # *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "\n",
    "        layers = []\n",
    "        layers.extend([\n",
    "            nn.Conv2d(inp, hidden_dim, kernel_size=(1,1), stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=(3,3), stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.Conv2d(hidden_dim, oup, kernel_size=(1,1), stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(oup),\n",
    "        ])\n",
    "\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.conv(x)\n",
    "\n",
    "class MobileNetV2_Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mobilenet = models.mobilenet_v2(weights=\"MobileNet_V2_Weights.IMAGENET1K_V1\")\n",
    "\n",
    "        for param in self.mobilenet.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for n in range(0, 2):\n",
    "            x = self.mobilenet.features[n](x)\n",
    "        x1 = x\n",
    "\n",
    "        for n in range(2, 4):\n",
    "            x = self.mobilenet.features[n](x)\n",
    "        x2 = x\n",
    "\n",
    "        for n in range(4, 7):\n",
    "            x = self.mobilenet.features[n](x)\n",
    "        x3 = x\n",
    "\n",
    "        return x1, x2, x3\n",
    "\n",
    "# UNet Decoder\n",
    "class UNet_Decoder(torch.nn.Module):\n",
    "    def __init__(self, n_classes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "        self.dconv1 = nn.ConvTranspose2d(32, 24, kernel_size=(4,4), stride=2, padding=1, bias=True)\n",
    "        self.invres1 = InvertedResidual(48, 24, stride=1, expand_ratio=6)\n",
    "\n",
    "        self.dconv2 = nn.ConvTranspose2d(24, 16, kernel_size=(4,4), stride=2, padding=1, bias=True)\n",
    "        self.invres2 = InvertedResidual(32, 16, stride=1, expand_ratio=6)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(16, 16, kernel_size=(1,1))\n",
    "        self.conv_score = nn.Conv2d(16, 3, kernel_size=(1,1))\n",
    "\n",
    "        self.sigmoid = torch.sigmoid\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2, x3 = x\n",
    "        # HINT:\n",
    "        # print(x1.shape, x2.shape, x3.shape)\n",
    "        # torch.Size([B, 16, 32, 32]) torch.Size([B, 24, 16, 16]) torch.Size([B, 32, 8, 8])\n",
    "        # x2.shape == self.dconv1(x3)\n",
    "        up3 = torch.cat([\n",
    "            x2,\n",
    "            self.dconv1(x3)\n",
    "        ], dim=1)\n",
    "        up3 = self.invres1(up3)\n",
    "\n",
    "        # x1.shape == self.dconv2(up3)\n",
    "        up4 = torch.cat([\n",
    "            x1,\n",
    "            self.dconv2(up3)\n",
    "        ], dim=1)\n",
    "        up4 = self.invres2(up4)\n",
    "\n",
    "        up5 = torch.nn.functional.interpolate(up4, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "        x = self.conv_last(up5)\n",
    "\n",
    "        x = self.conv_score(x)\n",
    "\n",
    "\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "You shouldn't need to change anything here, except for maybe the number of epochs. An example output plot is shown here:\n",
    "\n",
    "<center><img src=\"imgs/loss_and_accuracy.png\" alt=\"Plot\" width=\"960\" /></center>\n",
    "\n",
    "The network was trained for 50 epochs. However, this might take a while to train depending on your hardware. Though, as you can see, after 10 epochs we are already at around 75% accuracy. Which should be sufficient for early experiments, so you can quickly see wether you approach works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(model, loader, optimizer, loss_fn, device, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    return train_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, loss_fn, device, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x).squeeze(1)\n",
    "            loss = loss_fn(pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, pred = torch.max(pred, 1)\n",
    "            correct = torch.mean((pred == y).type(torch.float64))\n",
    "            print(\"-> Epoch: {:.1f}. Validation.  Accuracy: {:.3f}\".format(epoch, correct.cpu().numpy()))\n",
    "    return test_loss / len(loader), correct.cpu().numpy()\n",
    "\n",
    "def train_loop(model, train_loader, validation_loader, loss_fn, optimizer, device, epochs=10):\n",
    "    train_losses, test_losses, test_accuracy = [], [], []\n",
    "    for epoch in trange(epochs):\n",
    "        train_loss = train(model, train_loader, optimizer, loss_fn, device, epoch)\n",
    "        test_loss, accuracy = validate(model, validation_loader, loss_fn, device, epoch)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracy.append(accuracy)\n",
    "    return train_losses, test_losses, test_accuracy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "encoder = MobileNetV2_Encoder().to(device)\n",
    "decoder = UNet_Decoder().to(device)\n",
    "model = torch.nn.Sequential(encoder, decoder).to(device)\n",
    "model.to(device)\n",
    "\n",
    "# from torchinfo import summary\n",
    "# print(summary(encoder.mobilenet, input_size=(64, 3, 32, 32)))\n",
    "# print(summary(model, input_size=(64, 3, 32, 32)))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_losses, test_losses, test_accuracy = train_loop(model, train_loader, validation_loader, criterion, optimizer, device, epochs=2)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(test_losses, label=\"test\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(test_accuracy, label=\"test accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Evaluation\n",
    "\n",
    "In order to gain a deeper understanding of our network's performance, we will be analyzing a selection of its predictions through both qualitative and quantitative methods. Let's begin by focusing on the qualitative analysis, where we'll take a closer look at some visualizations of the predictions.\n",
    "\n",
    "For context, qualitative analysis involves evaluating the subjective aspects of data, such as visual appearance, whereas quantitative analysis involves examining objective data, such as numerical measurements or statistical analysis. By combining both approaches, we can gain a more comprehensive understanding of our model's strengths and limitations.\n",
    "\n",
    "### Qualitative Analysis\n",
    "\n",
    "The network should predict masks similiar in quality to this (However, some images are harder to predict and thus result in worse masks):\n",
    "\n",
    "<center><img src=\"imgs/network_output.png\" alt=\"Plot\" width=\"960\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y in validation_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "        _, pred = torch.max(pred, 1)\n",
    "        break\n",
    "\n",
    "for i in range(3):\n",
    "    # *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    # just put them on the same collumn\n",
    "    fig.add_subplot(1, 3, 1)\n",
    "    \n",
    "    # normalised it to get rid of the warning\n",
    "    img  = x[i].numpy().transpose((1, 2, 0))\n",
    "    img = img - np.min(img) \n",
    "    img = img / np.max(img)\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # truth\n",
    "    fig.add_subplot(1, 3, 2)\n",
    "    plt.imshow(y[i])\n",
    "    plt.axis('off')\n",
    "\n",
    "    # prediction\n",
    "    fig.add_subplot(1, 3, 3)\n",
    "    plt.imshow(pred[i])\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative Analysis\n",
    "\n",
    "In order to ensure the accuracy of our model on the test set, we will be implementing the Intersection over Union (IoU) metric. It's worth noting that in a real-world scenario, we would typically use a separate test set for this purpose. However, for the purposes of this exercise, we will be utilizing the validation set. A reasonable IoU to achive for this exercise is `0.45`, however, depending on the training time it may also be lower. In this case increasing the epochs might increase the IoU, especially, if the loss of the validation data is still decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(pred, target):\n",
    "    # *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    iou_list = []\n",
    "    for inputs, targets in validation_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)  # get predicted segmentation map\n",
    "\n",
    "        iou_val = iou(preds, targets)  # compute IoU\n",
    "        iou_list.append(iou_val)\n",
    "\n",
    "mean_iou = sum(iou_list) / len(iou_list)\n",
    "print('Mean IoU:', mean_iou)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcv_exercise_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
